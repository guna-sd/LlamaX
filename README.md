## LamaX:
LamaX is designed as an upgraded version of the [Llama](https://github.com/facebookresearch/llama).

## Sparse Attention Integration: 
LamaX uses [sparse attention](https://openai.com/research/sparse-transformer) to selectively attend to relevant parts of the input, reducing computational costs while maintaining high performance.
